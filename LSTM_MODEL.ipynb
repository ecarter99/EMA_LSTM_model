{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a756b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340982ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as fh:\n",
    "    config = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a85c17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to randomly sample the non/suicidal group windows based on #days\n",
    "    #Return:\n",
    "    #sample_encode: combination of non/suicidal randomly sampled encodings\n",
    "    #sample_lbl: combination of non/suicidal alligned random labels to encodings\n",
    "def sample_window(suicidal, nonsuicidal, batch_enc, batch_lbl, window=10):\n",
    "    sample_encode = []\n",
    "    sample_lbl = []\n",
    "    \n",
    "    for i in suicidal:\n",
    "        valid = False\n",
    "        while not valid:\n",
    "        \n",
    "            index_ls = [i for i, x in enumerate(batch_lbl[i]) if x==1 and i >= (window - 1)] #index of suicidal days\n",
    "\n",
    "            if not index_ls:\n",
    "                pos = random.randint(window ,len(batch_lbl[i]) - window - 1)\n",
    "                index_ls.append(pos)\n",
    "\n",
    "            rnd_idx = random.randint(0,len(index_ls) - 1) # random generation to choose an index from index_ls\n",
    "            index = index_ls[rnd_idx] #the index used (this is the label)\n",
    "            start = index - window - 1 #starting position of the sliding window for this particular survey run\n",
    "            \n",
    "            if (batch_enc[i][start:index+1]): break\n",
    "\n",
    "        sample_lbl.append([batch_lbl[i][index]])\n",
    "        sample_encode.append(batch_enc[i][start:index+1])\n",
    "    \n",
    "    for i in nonsuicidal:\n",
    "        valid = False\n",
    "        while not valid:\n",
    "        \n",
    "            index_ls = [i for i, x in enumerate(batch_lbl[i]) if x==0 and i >= (window - 1)] #index of suicidal days\n",
    "\n",
    "            rnd_idx = random.randint(0, len(index_ls) - 1) # random generation to choose an index from index_ls\n",
    "            index = index_ls[rnd_idx] #the index used (this is the label)\n",
    "            start = index - window - 1 #starting position of the sliding window for this particular survey run\n",
    "            \n",
    "            if (batch_enc[i][start:index+1]): break\n",
    "\n",
    "        sample_lbl.append([batch_lbl[i][index]])\n",
    "        sample_encode.append(batch_enc[i][start:index+1])\n",
    "    \n",
    "    #shuffle the lists but maintain coallation with the label\n",
    "    temp = list(zip(sample_encode, sample_lbl))\n",
    "    random.shuffle(temp)\n",
    "    shuf_encode, shuf_lbl = zip(*temp)\n",
    "    shuf_encode, shuf_lbl = list(shuf_encode), list(shuf_lbl)\n",
    "    \n",
    "    return shuf_encode, shuf_lbl\n",
    "\n",
    "#Function to Separate minibatches into non/suicidal groups to equally weigh\n",
    "#Key Question: Does this oversampling introduce too much bias?\n",
    "    #inputs:\n",
    "    #size: minibatch length to divide equally between samples\n",
    "    #Return:\n",
    "    #neg: participants with non-suicidal samples\n",
    "    #pos: participants with suicidal samples\n",
    "def oversample_classes(size):\n",
    "    indeces = random_selection(size, size)\n",
    "    pos = indeces[:int(size/2)]\n",
    "    neg = indeces[int(size/2):]\n",
    "    return pos, neg\n",
    "    \n",
    "\n",
    "#Random function with no repeats\n",
    "    #input:\n",
    "    #num_rnd = total random indeces needed (returned total list size)\n",
    "    #len_list = size of the list for random to know range for indeces\n",
    "    #Return:\n",
    "    #r_list = list of unique random indeces\n",
    "def random_selection(num_rnd, len_list):\n",
    "    rlist = []\n",
    "    for i in range(num_rnd):\n",
    "        r = random.randint(0, len_list-1)\n",
    "        while r in rlist: \n",
    "            r = random.randint(0, len_list-1)\n",
    "        rlist.append(r)\n",
    "    \n",
    "    return rlist\n",
    "            \n",
    "\n",
    "class AutismDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        # loading and preprocessing of ALL the data\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        #return later and make sure there is an offset\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # how to process just one example and one label\n",
    "        example = self.encodings[idx]\n",
    "        label = self.labels[idx]\n",
    "        return example, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def custom_collate_fn(minibatch):\n",
    "    batch_examples = list([list(e[0]) for e in minibatch]) #convert encodings to list\n",
    "    label_examples = list([list(e[1]) for e in minibatch]) #convert labels to list (need to track along with encod)\n",
    "    \n",
    "    max_len = max([len(e) for e in batch_examples]) #get the participant with most surveys\n",
    "    \n",
    "    mask = [] #setup the mask structure for all participants\n",
    "    for e in range(len(batch_examples)): #loop through all the participants\n",
    "        e_mask = [1 for i in range(len(batch_examples[e]))] #initialize this participants individual mask\n",
    "        while len(batch_examples[e]) < max_len: #looping over the longest number of surveys to mask/pad\n",
    "            e_mask.append(0) #adding to the mask for the particiapnt\n",
    "            label_examples[e].append(0) #padding label to track before collating\n",
    "            batch_examples[e].append([0 for i in range(37)]) #padding the participants survey data\n",
    "            \n",
    "        mask.append(e_mask) #adding the participants mask to the entirety of the mask data\n",
    "    \n",
    "    #randomly select the 2 classes of participants\n",
    "    negative, positive = oversample_classes(size=len(label_examples))\n",
    "    \n",
    "    #randomly sample x days within the classes\n",
    "    samp_batch, samp_lbl = sample_window(positive, negative, batch_examples, label_examples, 5)\n",
    "    \n",
    "    batch_examples = [torch.tensor(e) for e in samp_batch] #converting the endcodings to tensors\n",
    "    batch_labels = [torch.tensor(e).long() for e in samp_lbl] #converting labels to tensors\n",
    "    \n",
    "    return (batch_examples, batch_labels, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30f0fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_encodings(x, val_participants):\n",
    "    x = dataframe.loc[:, ('energy_levels','motivation','productivity','illness','food_health',\n",
    "                                      'physical_activity','leisure_time','negative_feelings','reduce_negativity',\n",
    "                                      'positive_feelings','increase_positive','enjoy_moment','show_feelings',\n",
    "                                      'accepting_feelings','fault_feeling','feel_better','feelings_last','stressed',\n",
    "                                      'stress_amount','stress_management','stress_interference','face_interaction',\n",
    "                                      'digital_interaction','person_connection','digital_connection',\n",
    "                                      'desire_interaction','feeling_support','spiritual_connection','number_naps',\n",
    "                                      'napping_time','days','Response_Gap','Next_Gap',\n",
    "                                      'suicide_thoughts','Recipient_First_Name','Prescribed_Group')]\n",
    "    \n",
    "    x[['stress_amount','stress_management',\n",
    "       'stress_interference','napping_time']] = x[['stress_amount','stress_management',\n",
    "                                                   'stress_interference','napping_time']].fillna(value=0)\n",
    "    x = x.dropna()\n",
    "    \n",
    "    x['reduce_negativity'] = x['reduce_negativity'].map({'Yes':1.0,'No':0.0})\n",
    "    x['increase_positive'] = x['increase_positive'].map({'Yes':1.0,'No':0.0})\n",
    "    x['enjoy_moment'] = x['enjoy_moment'].map({'Yes':1.0,'No':0.0})\n",
    "    x['show_feelings'] = x['show_feelings'].map({'Yes':1.0,'No':0.0})\n",
    "    x['accepting_feelings'] = x['accepting_feelings'].map({'Yes':1.0,'No':0.0})\n",
    "    x['fault_feeling'] = x['fault_feeling'].map({'Yes':1.0,'No':0.0})\n",
    "    x['feel_better'] = x['feel_better'].map({'Yes':1.0,'No':0.0})\n",
    "    x['feelings_last'] = x['feelings_last'].map({'Yes':1.0,'No':0.0})\n",
    "    x['stressed'] = x['stressed'].map({'Yes':1.0,'No':0.0})\n",
    "    x['suicide_thoughts'] = x['suicide_thoughts'].map({'Yes':1.0,'No':0.0})\n",
    "    x = x.drop(['napping_time'], axis=1)\n",
    "    \n",
    "    onehot_group = pd.get_dummies(x['Prescribed_Group'])\n",
    "    x = x.drop(['Prescribed_Group'],axis=1)\n",
    "    x = x.join(onehot_group)\n",
    "    \n",
    "    reshaped = []\n",
    "    labeled = []\n",
    "    \n",
    "    val_enc = []\n",
    "    val_lbl = []\n",
    "    for part in x.Recipient_First_Name.unique():\n",
    "        queried = x.query('Recipient_First_Name==@part')\n",
    "        \n",
    "        if part in val_participants:\n",
    "            val_lbl.append(queried.suicide_thoughts.values[1:])\n",
    "            queried = queried.drop('Recipient_First_Name',axis=1)\n",
    "            val_enc.append(queried.values[:-1])\n",
    "        else:\n",
    "            labeled.append(queried.suicide_thoughts.values[1:])\n",
    "            queried = queried.drop('Recipient_First_Name',axis=1)\n",
    "            reshaped.append(queried.values[:-1])\n",
    "        \n",
    "    return reshaped, labeled, val_enc, val_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5d92153",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('20230324_sorted_timed_evening.csv')\n",
    "#social anxious, autism, control (order for list)\n",
    "#58%, 26%, 0.6% suicidal respectively\n",
    "encodings, labels, validation_encoding, validation_lables = preprocess_encodings(dataframe,['EW91TS','YJ23HT','QO43HQ'])\n",
    "dataset = AutismDataset(encodings,labels)\n",
    "train_loader = torch.utils.data.DataLoader(dataset,collate_fn=custom_collate_fn,shuffle=True,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26be6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMmodel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer_size, num_layers, output_size, bidirectional=False, p_dropout=0.0):\n",
    "        super().__init__()\n",
    "           \n",
    "        self.num_features = num_features\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.output_size = output_size\n",
    "        self.p_dropout = p_dropout\n",
    "        self.hidden_layer_size = hidden_layer_size*(int(self.bidirectional)+1)\n",
    "        self.lstm = nn.LSTM(input_size=self.num_features, hidden_size=hidden_layer_size,\n",
    "                            num_layers=self.num_layers, batch_first=True, dropout=self.p_dropout,\n",
    "                            bidirectional=self.bidirectional)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_layer_size, self.hidden_layer_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.hidden_layer_size//2, self.output_size)\n",
    "                )\n",
    "\n",
    "    # \n",
    "    def forward(self, x):\n",
    "        bs, seq_len, _ = x.size()\n",
    "        hidden_cell = self.reset_forward(bs, x.device)\n",
    "        output, _ = self.lstm(x, hidden_cell)\n",
    "        final_hidden_representation = output[:,-1]\n",
    "        out = self.classifier(final_hidden_representation)\n",
    "        return out\n",
    "\n",
    "    # Function to reset the initial vector to 0's -> we don't want to muddy up the classification\n",
    "    def reset_forward(self, bs, device='cpu'):\n",
    "        return (torch.zeros((1+self.bidirectional)*self.num_layers,bs,int(self.hidden_layer_size/(1+self.bidirectional))).float().to(device),\n",
    "                torch.zeros((1+self.bidirectional)*self.num_layers,bs,int(self.hidden_layer_size/(1+self.bidirectional))).float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38b5208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMmodel(num_features=37, hidden_layer_size=128, num_layers=1, output_size=2, bidirectional=True, p_dropout=0)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "rate_learning = 1e-4\n",
    "optim = torch.optim.Adam(lstm.parameters(), lr=rate_learning, weight_decay=1e-3)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    \n",
    "    for encoding, label, mask in train_loader:\n",
    "        output = lstm(torch.stack(encoding).float())\n",
    "        loss = loss_function(output, torch.cat(label).long()) #if i pull out the 'labels' here...what can I replace it with?\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        running_loss += float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "397ef7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "tensor([0, 0, 1, 0, 0])\n",
      "label\n",
      "tensor([0, 0, 1, 0, 1])\n",
      "prediction\n",
      "tensor([0, 0, 0, 1, 0])\n",
      "label\n",
      "tensor([1, 1, 0, 0, 1])\n",
      "prediction\n",
      "tensor([1, 0, 0, 0, 0])\n",
      "label\n",
      "tensor([1, 0, 0, 1, 0])\n",
      "prediction\n",
      "tensor([1, 1, 1, 0, 0])\n",
      "label\n",
      "tensor([1, 1, 0, 0, 1])\n",
      "prediction\n",
      "tensor([0, 0, 1, 0, 0])\n",
      "label\n",
      "tensor([0, 0, 1, 1, 0])\n",
      "prediction\n",
      "tensor([0, 1, 0, 0, 1])\n",
      "label\n",
      "tensor([0, 1, 0, 0, 1])\n",
      "prediction\n",
      "tensor([1, 0, 0, 1, 0])\n",
      "label\n",
      "tensor([1, 0, 0, 1, 0])\n",
      "prediction\n",
      "tensor([1, 0, 0, 1, 0])\n",
      "label\n",
      "tensor([1, 1, 0, 1, 0])\n",
      "prediction\n",
      "tensor([0, 0, 1, 0, 1])\n",
      "label\n",
      "tensor([0, 0, 1, 0, 1])\n",
      "prediction\n",
      "tensor([1, 1, 1, 0, 0])\n",
      "label\n",
      "tensor([1, 0, 1, 1, 0])\n",
      "prediction\n",
      "tensor([1, 0, 0, 0, 0])\n",
      "label\n",
      "tensor([0, 0, 0, 0, 1])\n",
      "prediction\n",
      "tensor([0, 1, 0, 0, 1])\n",
      "label\n",
      "tensor([0, 1, 0, 0, 0])\n",
      "prediction\n",
      "tensor([1, 0, 1, 0, 0])\n",
      "label\n",
      "tensor([1, 0, 1, 0, 0])\n",
      "prediction\n",
      "tensor([1, 0, 1, 1, 0])\n",
      "label\n",
      "tensor([1, 0, 0, 1, 0])\n",
      "prediction\n",
      "tensor([0, 1, 1, 0, 1])\n",
      "label\n",
      "tensor([0, 1, 1, 1, 0])\n",
      "prediction\n",
      "tensor([0, 0, 0, 0])\n",
      "label\n",
      "tensor([0, 1, 0, 1])\n",
      "Accuracy of the network: 74.683544 %\n"
     ]
    }
   ],
   "source": [
    "testset = AutismDataset(validation_encoding, validation_lables)\n",
    "testloader = torch.utils.data.DataLoader(dataset,collate_fn=custom_collate_fn,shuffle=True,batch_size=5)\n",
    "\n",
    "torch.save({'state_dict': lstm.state_dict(),\n",
    "                    'optimizer' : optim.state_dict(),\n",
    "                   }, 'first_model.pth')\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "test_encodings, test_labels, test_mask = next(dataiter)\n",
    "\n",
    "outputs = lstm(torch.stack(test_encodings).float())\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels, mask = data\n",
    "        outputs = lstm(torch.stack(images).float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += torch.tensor(labels).size(0)\n",
    "        labels = torch.cat(labels).long()\n",
    "        print('prediction')\n",
    "        print(predicted)\n",
    "        print('label')\n",
    "        print(labels)\n",
    "        correct += float((predicted == labels).sum().item())\n",
    "\n",
    "print('Accuracy of the network: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af23dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
